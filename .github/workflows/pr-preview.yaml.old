# .github/workflows/pr-preview.yml
name: PR Preview Environment

on:
  pull_request:
    types: [opened, synchronize, reopened, closed]
    paths:
      - "apps/**"
      - "clusters/homelab/03-community/**"

permissions:
  contents: read
  issues: write
  pull-requests: write
  actions: read

env:
  DOMAIN_GENERATOR_URL: "https://dg.homelab.local"
  # Note: INTERNAL_SERVICE_IP not needed since we're using service-to-service communication

jobs:
  detect-changes:
    runs-on: self-hosted
    outputs:
      services: ${{ steps.changes.outputs.services }}
      has-changes: ${{ steps.changes.outputs.has-changes }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changed services
        id: changes
        run: |
          # Detect which services were changed
          CHANGED_FILES=$(git diff --name-only origin/main...HEAD)
          echo "Changed files: $CHANGED_FILES"

          # Extract actual service names from changed paths - handles nested structure
          # Pattern: apps/category/service/... -> extract "category/service"
          SERVICES=$(echo "$CHANGED_FILES" | grep -E '^apps/[^/]+/[^/]+/' | cut -d'/' -f2,3 | sort | uniq | tr '\n' ' ')
          COMMUNITY_REFS=$(echo "$CHANGED_FILES" | grep -E '^clusters/homelab/03-community/' | sed 's|.*/||' | sed 's|\.yaml$||' | tr '\n' ' ')

          # Combine and deduplicate
          ALL_SERVICES=$(echo "$SERVICES $COMMUNITY_REFS" | tr ' ' '\n' | sort | uniq | tr '\n' ' ')

          # Remove leading/trailing spaces
          ALL_SERVICES_TRIMMED=$(echo "$ALL_SERVICES" | sed 's/^ *//;s/ *$//')

          echo "services=$ALL_SERVICES_TRIMMED" >> $GITHUB_OUTPUT
          if [ -n "$ALL_SERVICES_TRIMMED" ]; then
            echo "has-changes=true" >> $GITHUB_OUTPUT
          else
            echo "has-changes=false" >> $GITHUB_OUTPUT
          fi

          echo "Detected services: $ALL_SERVICES_TRIMMED"

  preview-deploy:
    runs-on: self-hosted
    needs: detect-changes
    if: github.event.action != 'closed' && needs.detect-changes.outputs.has-changes == 'true'

    steps:
      - uses: actions/checkout@v4

      - name: Deploy services to preview
        run: |
          SERVICES="${{ needs.detect-changes.outputs.services }}"
          PR_NUMBER="${{ github.event.number }}"
          BRANCH_NAME="${{ github.head_ref }}"

          echo "Services to deploy: $SERVICES"
          echo "PR Number: $PR_NUMBER"
          echo "Branch: $BRANCH_NAME"

          # Initialize temp file for domain info
          > /tmp/preview-domains.txt

          # Process each service (now in format "category/service")
          for SERVICE_PATH in $SERVICES; do
            echo "========================================"
            echo "Processing service path: $SERVICE_PATH"
            echo "========================================"
            
            # Extract category and service name
            CATEGORY=$(echo "$SERVICE_PATH" | cut -d'/' -f1)
            SERVICE_NAME=$(echo "$SERVICE_PATH" | cut -d'/' -f2)
            
            # Create sanitized namespace name (replace / with -)
            SERVICE_NAMESPACE_NAME=$(echo "$SERVICE_PATH" | tr '/' '-')
            PREVIEW_NAMESPACE="preview-pr-$PR_NUMBER-$SERVICE_NAMESPACE_NAME"
            
            echo "Category: $CATEGORY"
            echo "Service Name: $SERVICE_NAME"
            echo "Namespace: $PREVIEW_NAMESPACE"
            
            # Validate service structure
            echo "Validating service: $SERVICE_PATH"
            if [ ! -d "apps/$SERVICE_PATH" ]; then
              echo "‚ùå Service directory apps/$SERVICE_PATH not found"
              continue
            fi

            # Check for base directory
            if [ -d "apps/$SERVICE_PATH/base" ]; then
              echo "‚úÖ Found base directory at apps/$SERVICE_PATH/base"
              BASE_DIR="apps/$SERVICE_PATH/base"
            else
              echo "‚ùå No base directory found at apps/$SERVICE_PATH/base"
              continue
            fi

            # Check for overlay directory
            OVERLAY_DIR=""
            if [ -d "apps/$SERVICE_PATH/overlays/dev" ]; then
              echo "‚úÖ Found dev overlay at apps/$SERVICE_PATH/overlays/dev"
              OVERLAY_DIR="apps/$SERVICE_PATH/overlays/dev"
            else
              echo "‚ö†Ô∏è No dev overlay found, will use base only"
            fi

            # Validate Kubernetes manifests
            echo "Validating base manifests..."
            find "$BASE_DIR" -name "*.yaml" -not -name "kustomization.yaml" -exec kubectl apply --dry-run=client -f {} \; || {
              echo "‚ùå Base manifest validation failed for $SERVICE_PATH"
              continue
            }

            if [ -n "$OVERLAY_DIR" ]; then
              echo "Validating dev overlay..."
              cd "$OVERLAY_DIR"
              if kustomize build . | kubectl apply --dry-run=client -f -; then
                echo "‚úÖ Dev overlay validation passed"
              else
                echo "‚ùå Dev overlay validation failed for $SERVICE_PATH"
                cd - > /dev/null
                continue
              fi
              cd - > /dev/null
            fi
            
            # Deploy to preview environment
            echo "Deploying $SERVICE_PATH to preview namespace: $PREVIEW_NAMESPACE"

            # Create preview namespace
            kubectl create namespace $PREVIEW_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

            # Wait for namespace to be ready
            echo "Waiting for namespace to be ready..."
            kubectl wait --for=condition=Ready namespace/$PREVIEW_NAMESPACE --timeout=30s || {
              echo "‚ö†Ô∏è Namespace may not be ready, but continuing..."
            }

            # Label for easy cleanup
            kubectl label namespace $PREVIEW_NAMESPACE \
              preview=true \
              pr-number=$PR_NUMBER \
              service="$SERVICE_NAMESPACE_NAME" \
              created-by=github-actions

            # Deploy service to preview namespace
            echo "Deploying manifests to namespace: $PREVIEW_NAMESPACE"
            
            DEPLOYMENT_SUCCESS=false
            DEPLOYMENT_ERROR=""
            
            if [ -n "$OVERLAY_DIR" ]; then
              echo "Using kustomize overlay for deployment..."
              cd "$OVERLAY_DIR"
              
              # Build with kustomize and apply to specific namespace
              KUSTOMIZE_OUTPUT=$(kustomize build . 2>&1)
              if [ $? -eq 0 ]; then
                echo "‚úÖ Kustomize build successful"
                
                # Apply the built manifests
                APPLY_OUTPUT=$(echo "$KUSTOMIZE_OUTPUT" | yq eval 'del(.metadata.namespace)' - | kubectl apply -f - -n $PREVIEW_NAMESPACE 2>&1)
                if [ $? -eq 0 ]; then
                  echo "‚úÖ Kustomize deployment successful"
                  DEPLOYMENT_SUCCESS=true
                else
                  echo "‚ùå Kustomize deployment failed"
                  DEPLOYMENT_ERROR="Kustomize apply failed: $APPLY_OUTPUT"
                fi
              else
                echo "‚ùå Kustomize build failed"
                DEPLOYMENT_ERROR="Kustomize build failed: $KUSTOMIZE_OUTPUT"
              fi
              
              cd - > /dev/null
            else
              echo "Using base manifests for deployment..."
              echo "=== Applying base manifests ==="
              
              MANIFEST_ERRORS=""
              APPLIED_COUNT=0
              TOTAL_COUNT=0
              
              # Find all YAML files in base directory
              while IFS= read -r -d '' manifest_file; do
                TOTAL_COUNT=$((TOTAL_COUNT + 1))
                echo "Applying: $manifest_file"
                
                APPLY_OUTPUT=$(kubectl apply -f "$manifest_file" -n $PREVIEW_NAMESPACE 2>&1)
                if [ $? -eq 0 ]; then
                  echo "‚úÖ Applied $manifest_file successfully"
                  APPLIED_COUNT=$((APPLIED_COUNT + 1))
                else
                  echo "‚ùå Failed to apply $manifest_file"
                  MANIFEST_ERRORS="$MANIFEST_ERRORS\n- $(basename "$manifest_file"): $APPLY_OUTPUT"
                fi
              done < <(find "$BASE_DIR" -name "*.yaml" -not -name "kustomization.yaml" -print0)
              
              if [ $APPLIED_COUNT -eq $TOTAL_COUNT ] && [ $TOTAL_COUNT -gt 0 ]; then
                echo "‚úÖ All base manifests applied successfully ($APPLIED_COUNT/$TOTAL_COUNT)"
                DEPLOYMENT_SUCCESS=true
              elif [ $APPLIED_COUNT -gt 0 ]; then
                echo "‚ö†Ô∏è Partial deployment success ($APPLIED_COUNT/$TOTAL_COUNT)"
                DEPLOYMENT_ERROR="Partial deployment failure. Applied $APPLIED_COUNT out of $TOTAL_COUNT manifests. Errors:$MANIFEST_ERRORS"
              else
                echo "‚ùå All base manifest deployments failed"
                DEPLOYMENT_ERROR="All manifest deployments failed. Errors:$MANIFEST_ERRORS"
              fi
            fi

            # Check deployment status before proceeding
            if [ "$DEPLOYMENT_SUCCESS" = false ]; then
              echo "üí• DEPLOYMENT FAILED - Skipping domain generation"
              echo "Error details: $DEPLOYMENT_ERROR"
              
              # Store failure info for PR comment
              echo "$SERVICE_PATH|DEPLOYMENT_FAILED|$DEPLOYMENT_ERROR|||" >> /tmp/preview-domains.txt
              
              # Show namespace events for debugging
              echo "=== Namespace Events (for debugging) ==="
              kubectl get events -n $PREVIEW_NAMESPACE --sort-by='.lastTimestamp' | tail -10 || echo "No events found"
              
              # Continue to next service (don't exit completely)
              echo "‚è≠Ô∏è Moving to next service..."
              continue
            fi

            # Check if resources were actually created
            echo "=== Checking created resources ==="
            sleep 10  # Give more time for resources to be created
            kubectl get all -n $PREVIEW_NAMESPACE
            
            # Verify that pods are running
            echo "=== Verifying pod status ==="
            POD_STATUS=$(kubectl get pods -n $PREVIEW_NAMESPACE --no-headers 2>/dev/null)
            if [ -n "$POD_STATUS" ]; then
              echo "Pod status:"
              echo "$POD_STATUS"
              
              # Check for failed pods
              FAILED_PODS=$(echo "$POD_STATUS" | grep -E "(Error|CrashLoopBackOff|ImagePullBackOff|Failed)" || true)
              if [ -n "$FAILED_PODS" ]; then
                echo "‚ö†Ô∏è Found failed pods:"
                echo "$FAILED_PODS"
                
                # Get pod logs for debugging
                echo "=== Failed Pod Logs ==="
                kubectl get pods -n $PREVIEW_NAMESPACE --no-headers | grep -E "(Error|CrashLoopBackOff|ImagePullBackOff|Failed)" | while read pod_line; do
                  POD_NAME=$(echo "$pod_line" | awk '{print $1}')
                  echo "--- Logs for $POD_NAME ---"
                  kubectl logs "$POD_NAME" -n $PREVIEW_NAMESPACE --tail=20 2>/dev/null || echo "Could not retrieve logs"
                done
              fi
            else
              echo "‚ö†Ô∏è No pods found in namespace"
            fi
            
            # Extract service information from deployed resources (only if deployment succeeded)
            echo "=== Extracting service information from manifests ==="
            
            # Get the actual service from the deployed resources
            SERVICE_INFO=$(kubectl get service -n $PREVIEW_NAMESPACE -o json 2>/dev/null | jq -r '.items[0] // empty')
            
            if [ -n "$SERVICE_INFO" ] && [ "$SERVICE_INFO" != "null" ]; then
              # Extract service details from deployed service
              ACTUAL_SERVICE_NAME=$(echo "$SERVICE_INFO" | jq -r '.metadata.name')
              ACTUAL_SERVICE_PORT=$(echo "$SERVICE_INFO" | jq -r '.spec.ports[0].port // 80')
              ACTUAL_TARGET_PORT=$(echo "$SERVICE_INFO" | jq -r '.spec.ports[0].targetPort // .spec.ports[0].port // 80')
              
              echo "‚úÖ Found deployed service:"
              echo "  - Service Name: $ACTUAL_SERVICE_NAME"
              echo "  - Service Port: $ACTUAL_SERVICE_PORT"
              echo "  - Target Port: $ACTUAL_TARGET_PORT"
              
              # Construct internal service URL using actual service details
              INTERNAL_SERVICE_URL="http://${ACTUAL_SERVICE_NAME}.${PREVIEW_NAMESPACE}:${ACTUAL_SERVICE_PORT}"
              
            else
              echo "‚ö†Ô∏è No service found in deployed resources, checking manifests directly..."
              
              # Fallback: Parse manifests to extract service info
              if [ -n "$OVERLAY_DIR" ]; then
                cd "$OVERLAY_DIR"
                MANIFEST_CONTENT=$(kustomize build . 2>/dev/null || echo "")
                cd - > /dev/null
              else
                # Combine all base manifests
                MANIFEST_CONTENT=""
                find "$BASE_DIR" -name "*.yaml" -not -name "kustomization.yaml" | while read -r manifest_file; do
                  echo "---" >> /tmp/combined_manifest.yaml
                  cat "$manifest_file" >> /tmp/combined_manifest.yaml
                done
                MANIFEST_CONTENT=$(cat /tmp/combined_manifest.yaml 2>/dev/null || echo "")
                rm -f /tmp/combined_manifest.yaml
              fi
              
              if [ -n "$MANIFEST_CONTENT" ]; then
                # Extract service info from manifest content
                SERVICE_MANIFEST=$(echo "$MANIFEST_CONTENT" | yq eval 'select(.kind == "Service")' -)
                
                if [ -n "$SERVICE_MANIFEST" ] && [ "$SERVICE_MANIFEST" != "null" ]; then
                  ACTUAL_SERVICE_NAME=$(echo "$SERVICE_MANIFEST" | yq eval '.metadata.name' -)
                  ACTUAL_SERVICE_PORT=$(echo "$SERVICE_MANIFEST" | yq eval '.spec.ports[0].port // 80' -)
                  
                  echo "‚úÖ Found service in manifests:"
                  echo "  - Service Name: $ACTUAL_SERVICE_NAME"
                  echo "  - Service Port: $ACTUAL_SERVICE_PORT"
                  
                  # Construct internal service URL
                  INTERNAL_SERVICE_URL="http://${ACTUAL_SERVICE_NAME}.${PREVIEW_NAMESPACE}:${ACTUAL_SERVICE_PORT}"
                else
                  echo "‚ùå No service found in manifests - this deployment may be incomplete"
                  DEPLOYMENT_ERROR="No Kubernetes Service found in manifests. The application may not be accessible externally."
                  
                  # Store failure info
                  echo "$SERVICE_PATH|NO_SERVICE_FOUND|$DEPLOYMENT_ERROR|||" >> /tmp/preview-domains.txt
                  continue
                fi
              else
                echo "‚ùå Could not read manifest content"
                DEPLOYMENT_ERROR="Could not parse manifests to extract service information."
                
                # Store failure info
                echo "$SERVICE_PATH|MANIFEST_PARSE_ERROR|$DEPLOYMENT_ERROR|||" >> /tmp/preview-domains.txt
                continue
              fi
            fi
            
            echo "Final internal service URL: $INTERNAL_SERVICE_URL"
            
            # Wait for service to be ready
            echo "=== Waiting for service to be ready ==="
            SERVICE_READY=false
            for i in {1..12}; do  # Wait up to 2 minutes
              if kubectl get service "$ACTUAL_SERVICE_NAME" -n $PREVIEW_NAMESPACE >/dev/null 2>&1; then
                # Check if service has endpoints
                ENDPOINTS=$(kubectl get endpoints "$ACTUAL_SERVICE_NAME" -n $PREVIEW_NAMESPACE -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || echo "")
                if [ -n "$ENDPOINTS" ]; then
                  echo "‚úÖ Service is ready with endpoints: $ENDPOINTS"
                  SERVICE_READY=true
                  break
                else
                  echo "‚è≥ Service exists but no endpoints yet (attempt $i/12)"
                fi
              else
                echo "‚è≥ Waiting for service to be created (attempt $i/12)"
              fi
              sleep 10
            done
            
            if [ "$SERVICE_READY" = false ]; then
              echo "‚ö†Ô∏è Service may not be fully ready, but proceeding with domain generation"
              # Don't fail completely, but add warning to the comment
              SERVICE_WARNING="Service may not be fully ready - please verify deployment status"
            fi
            
            # Wait for deployment to be ready
            echo "Waiting for deployment to be ready..."
            if kubectl get deployment -n $PREVIEW_NAMESPACE --no-headers 2>/dev/null | grep -q .; then
              kubectl wait --for=condition=available deployment --all -n $PREVIEW_NAMESPACE --timeout=300s || {
                echo "‚ö†Ô∏è Deployment may not be ready yet, but continuing..."
              }
            else
              echo "‚ö†Ô∏è No deployments found in namespace, skipping wait"
            fi
            
            # Generate preview domain with extracted service information
            echo "Generating domain for service: $SERVICE_NAMESPACE_NAME"
            echo "Using extracted service details:"
            echo "  - Service Name: $ACTUAL_SERVICE_NAME"
            echo "  - Namespace: $PREVIEW_NAMESPACE" 
            echo "  - Internal URL: $INTERNAL_SERVICE_URL"
            
            DOMAIN_RESPONSE=$(curl -s -X POST "$DOMAIN_GENERATOR_URL/api/domains/generate" \
              -H "Content-Type: application/json" \
              -d "{
                \"serviceName\": \"$SERVICE_NAMESPACE_NAME\",
                \"namespace\": \"$PREVIEW_NAMESPACE\",
                \"internalService\": \"${ACTUAL_SERVICE_NAME}.${PREVIEW_NAMESPACE}\",
                \"port\": $ACTUAL_SERVICE_PORT,
                \"useZeroTrust\": true,
                \"pullRequestId\": \"$PR_NUMBER\",
                \"branch\": \"$BRANCH_NAME\"
              }")

            echo "Domain API Response: $DOMAIN_RESPONSE"

            # Parse response
            SUCCESS=$(echo "$DOMAIN_RESPONSE" | jq -r '.success // false')
            PREVIEW_DOMAIN=$(echo "$DOMAIN_RESPONSE" | jq -r '.domain.full_domain // empty')
            PREVIEW_URL=$(echo "$DOMAIN_RESPONSE" | jq -r '.domain.url // empty')

            if [ "$SUCCESS" = "true" ] && [ -n "$PREVIEW_DOMAIN" ]; then
              DOMAIN_ID=$(echo "$DOMAIN_RESPONSE" | jq -r '.domain.id // empty')
              echo "‚úÖ Preview domain created: $PREVIEW_DOMAIN"
              echo "‚úÖ Preview URL: $PREVIEW_URL"
              
              # Store domain info with actual service details and any warnings
              DOMAIN_INFO_LINE="$SERVICE_PATH|$PREVIEW_DOMAIN|$PREVIEW_URL|$DOMAIN_ID|$ACTUAL_SERVICE_NAME|$ACTUAL_SERVICE_PORT"
              if [ -n "$SERVICE_WARNING" ]; then
                DOMAIN_INFO_LINE="$DOMAIN_INFO_LINE|$SERVICE_WARNING"
              fi
              echo "$DOMAIN_INFO_LINE" >> /tmp/preview-domains.txt
              
              kubectl label namespace $PREVIEW_NAMESPACE \
                domain-id=$DOMAIN_ID \
                --overwrite
            else
              echo "‚ùå Failed to create preview domain for $SERVICE_PATH"
              FALLBACK_INFO_LINE="$SERVICE_PATH|pr-$PR_NUMBER-$SERVICE_NAMESPACE_NAME.abdullahainun.site|https://pr-$PR_NUMBER-$SERVICE_NAMESPACE_NAME.abdullahainun.site||$ACTUAL_SERVICE_NAME|$ACTUAL_SERVICE_PORT"
              if [ -n "$SERVICE_WARNING" ]; then
                FALLBACK_INFO_LINE="$FALLBACK_INFO_LINE|$SERVICE_WARNING"
              fi
              echo "$FALLBACK_INFO_LINE" >> /tmp/preview-domains.txt
            fi
            
            echo "‚úÖ Completed deployment for service: $SERVICE_PATH"
            echo ""
          done

  comment-preview:
    runs-on: self-hosted
    needs: [detect-changes, preview-deploy]
    if: always() && github.event.action != 'closed' && needs.detect-changes.outputs.has-changes == 'true'

    steps:
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const prNumber = ${{ github.event.number }};

            let comment = `## üöÄ Preview Environment Status\n\n`;

            let hasSuccessfulDeployments = false;
            let hasFailedDeployments = false;

            // Read domain info from temp file
            let domainInfo = [];
            try {
              const domainData = fs.readFileSync('/tmp/preview-domains.txt', 'utf8');
              domainInfo = domainData.trim().split('\n').filter(line => line.length > 0);
              console.log(`Found ${domainInfo.length} domain info entries`);
              console.log('Domain info content:', domainData);
            } catch (error) {
              console.log('No domain info file found:', error.message);
              comment += `### ‚ö†Ô∏è No Deployment Information Found\n\n`;
              comment += `Could not find deployment status information. This might indicate:\n`;
              comment += `- No services were detected in the changed files\n`;
              comment += `- The deployment process didn't complete\n`;
              comment += `- There was an issue with the workflow execution\n\n`;
              comment += `Please check the workflow logs for more details.`;
              
              github.rest.issues.createComment({
                issue_number: prNumber,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              return;
            }

            // Handle case where file exists but is empty
            if (domainInfo.length === 0) {
              console.log('Domain info file is empty');
              comment += `### ‚ö†Ô∏è No Services Were Processed\n\n`;
              comment += `The deployment workflow ran but no services were processed. This might indicate:\n`;
              comment += `- No valid service directories were found in the changed files\n`;
              comment += `- All services failed validation before deployment\n`;
              comment += `- The service detection logic didn't match your directory structure\n\n`;
              comment += `Please check the workflow logs for more details about service detection.`;
              
              github.rest.issues.createComment({
                issue_number: prNumber,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              return;
            }

            for (const line of domainInfo) {
              const parts = line.split('|');
              const [servicePath, domain, url, domainId, actualServiceName, servicePort, warning] = parts;
              
              if (servicePath) {
                if (domain === 'DEPLOYMENT_FAILED' || domain === 'NO_SERVICE_FOUND' || domain === 'MANIFEST_PARSE_ERROR' || domain === 'DOMAIN_API_ERROR') {
                  // Handle deployment failures
                  hasFailedDeployments = true;
                  const errorType = domain;
                  const errorDetails = url || 'Unknown error';
                  
                  comment += `### ‚ùå ${servicePath} - Deployment Failed\n`;
                  comment += `**Error Type:** ${errorType.replace(/_/g, ' ').toLowerCase()}\n\n`;
                  comment += `**Error Details:**\n`;
                  comment += `\`\`\`\n${errorDetails}\`\`\`\n\n`;
                  comment += `**Troubleshooting:**\n`;
                  
                  if (errorType === 'DEPLOYMENT_FAILED') {
                    comment += `- Check your Kubernetes manifests for syntax errors\n`;
                    comment += `- Verify resource limits and requests\n`;
                    comment += `- Check for missing dependencies or secrets\n`;
                    comment += `- Review the manifest validation logs above\n\n`;
                  } else if (errorType === 'NO_SERVICE_FOUND') {
                    comment += `- Add a Kubernetes Service resource to your manifests\n`;
                    comment += `- Ensure the Service selector matches your pod labels\n`;
                    comment += `- Verify the Service is defined in base/ or overlay/\n\n`;
                  } else if (errorType === 'MANIFEST_PARSE_ERROR') {
                    comment += `- Check YAML syntax in your manifests\n`;
                    comment += `- Ensure kustomization.yaml is properly configured\n`;
                    comment += `- Verify file paths in kustomization resources\n\n`;
                  } else if (errorType === 'DOMAIN_API_ERROR') {
                    comment += `- Check if the domain generator service is running\n`;
                    comment += `- Verify the domain generator API configuration\n`;
                    comment += `- Check for Cloudflare API token issues\n`;
                    comment += `- Review the domain generator service logs\n\n`;
                  }
                  
                } else if (domain && domain !== '') {
                  // Handle successful deployments
                  hasSuccessfulDeployments = true;
                  const namespaceFormatted = servicePath.replace('/', '-');
                  const previewNamespace = `preview-pr-${prNumber}-${namespaceFormatted}`;
                  
                  comment += `### ‚úÖ ${servicePath} - Deployed Successfully\n`;
                  comment += `- **Preview URL:** ${url || `https://${domain}`}\n`;
                  comment += `- **Domain:** \`${domain}\`\n`;
                  comment += `- **Namespace:** \`${previewNamespace}\`\n`;
                  
                  // Show actual service details from manifests
                  if (actualServiceName && servicePort) {
                    comment += `- **Service Name:** \`${actualServiceName}\`\n`;
                    comment += `- **Service Port:** \`${servicePort}\`\n`;
                    comment += `- **Internal Service:** \`http://${actualServiceName}.${previewNamespace}:${servicePort}\`\n`;
                  }
                  
                  // Show warnings if any
                  if (warning && warning.trim() !== '') {
                    comment += `- **‚ö†Ô∏è Warning:** ${warning}\n`;
                  }
                  
                  comment += `- **Auto-updates:** ‚úÖ (on new commits)\n`;
                  comment += `- **Expires:** 24 hours\n\n`;
                }
              }
            }

            // Add summary section
            if (hasSuccessfulDeployments && hasFailedDeployments) {
              comment += `### üìä Deployment Summary\n`;
              comment += `- ‚úÖ Some services deployed successfully\n`;
              comment += `- ‚ùå Some services failed to deploy\n`;
              comment += `- üîß Please fix the failed deployments and push again\n\n`;
            } else if (hasFailedDeployments) {
              comment += `### üìä Deployment Summary\n`;
              comment += `- ‚ùå All deployments failed\n`;
              comment += `- üîß Please fix the issues and push again\n`;
              comment += `- üí° Check the error details above for troubleshooting\n\n`;
            } else if (hasSuccessfulDeployments) {
              comment += `### üìä Deployment Summary\n`;
              comment += `- ‚úÖ All services deployed successfully!\n`;
              comment += `- üéâ Your preview environments are ready\n\n`;
            }

            if (hasSuccessfulDeployments) {
              comment += `### ‚ÑπÔ∏è Preview Info\n`;
              comment += `- üîÑ **Auto-updates** on every commit to this PR\n`;
              comment += `- üïê **Auto-expires** after 24 hours\n`;
              comment += `- üßπ **Auto-cleanup** when PR is closed\n`;
              comment += `- üîí **Zero Trust** protection enabled\n`;
              comment += `- üìä **Resource limits** applied for fair usage\n\n`;
            }

            comment += `*Preview environments are automatically managed by our GitOps platform*`;

            // Create the GitHub comment
            try {
              await github.rest.issues.createComment({
                issue_number: prNumber,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              console.log('Successfully created PR comment');
            } catch (error) {
              console.error('Failed to create PR comment:', error.message);
              throw error;
            }

  cleanup-preview:
    runs-on: self-hosted
    if: github.event.action == 'closed'

    steps:
      - name: Cleanup preview environments
        run: |
          PR_NUMBER="${{ github.event.number }}"

          echo "Cleaning up preview environments for PR #$PR_NUMBER"

          # Find all preview namespaces for this PR
          PREVIEW_NAMESPACES=$(kubectl get namespaces -l pr-number=$PR_NUMBER -o name 2>/dev/null || true)

          if [ -n "$PREVIEW_NAMESPACES" ]; then
            echo "Found preview namespaces: $PREVIEW_NAMESPACES"
            
            # Extract domain IDs and delete domains first
            for ns in $PREVIEW_NAMESPACES; do
              NAMESPACE_NAME=$(echo $ns | cut -d'/' -f2)
              echo "Processing namespace: $NAMESPACE_NAME"
              
              DOMAIN_ID=$(kubectl get namespace $NAMESPACE_NAME -o jsonpath='{.metadata.labels.domain-id}' 2>/dev/null || echo "")
              
              if [ -n "$DOMAIN_ID" ] && [ "$DOMAIN_ID" != "" ]; then
                echo "Deleting domain ID: $DOMAIN_ID"
                CLEANUP_RESPONSE=$(curl -s -X DELETE "$DOMAIN_GENERATOR_URL/api/domains/$DOMAIN_ID" \
                  -H "Content-Type: application/json")
                echo "Domain deletion response: $CLEANUP_RESPONSE"
              fi
              
              echo "Deleting namespace: $ns"
              kubectl delete $ns --ignore-not-found=true
            done
          else
            echo "No preview namespaces found for PR #$PR_NUMBER"
          fi

      - name: Comment cleanup
        uses: actions/github-script@v7
        with:
          script: |
            const prNumber = ${{ github.event.number }};
            const comment = `## üßπ Preview Environment Cleaned Up\n\n` +
              `All preview resources for this PR have been automatically removed:\n\n` +
              `- ‚úÖ Preview namespaces deleted\n` +
              `- ‚úÖ Preview domains removed\n` +
              `- ‚úÖ All resources cleaned up\n\n` +
              `Thank you for contributing! üôè`;

            github.rest.issues.createComment({
              issue_number: prNumber,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  production-deploy:
    runs-on: self-hosted
    needs: detect-changes
    if: github.event.action == 'closed' && github.event.pull_request.merged == true && needs.detect-changes.outputs.has-changes == 'true'

    steps:
      - name: Trigger production deployment
        run: |
          echo "PR merged! Production deployment will be handled by Flux CD"
          echo "Services will be deployed via regular GitOps flow"

      - name: Comment production deployment
        uses: actions/github-script@v7
        with:
          script: |
            const prNumber = ${{ github.event.number }};
            const comment = `## üéâ Merged to Production!\n\n` +
              `Your contribution has been merged and will be deployed to production via Flux CD.\n\n` +
              `### What happens next:\n` +
              `- üîÑ Flux CD will deploy your service to production namespace\n` +
              `- üåê Permanent domain will be generated\n` +
              `- üìä Service will appear in the public catalog\n` +
              `- ‚ú® Community can discover and use your service\n\n` +
              `**Estimated production deployment time:** 2-5 minutes\n\n` +
              `Thank you for your contribution! üöÄ`;

            github.rest.issues.createComment({
              issue_number: prNumber,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
